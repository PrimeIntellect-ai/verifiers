model = "willcb/Qwen3-0.6B"


[inference]
gpus = 1

[inference.args] # vllm args
enforce_eager = true
tensor_parallel_size = 1

[trainer]
gpus = 1
run_name = "gsm8k"

[trainer.args] # rl args
micro_batch_size = 12
rollouts_per_example = 12
batch_size = 96
max_steps = 100
eval_strategy = "steps"
eval_steps = 10
max_tokens = 1024
max_seq_len = 2048
