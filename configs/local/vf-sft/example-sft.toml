# Example SFT training configuration
# Usage: vf-sft @ configs/local/vf-sft/example-sft.toml

model = "Qwen/Qwen3-4B-Instruct-2507"
dataset = "willcb/V3-wordle"

[sft]
# Experiment name
run_name = "wordle-sft-example"

# Training parameters
max_steps = 500
learning_rate = 2e-5
num_train_epochs = 1
weight_decay = 0.0
max_grad_norm = 1.0

# Batch parameters
batch_size = 512
micro_batch_size = 8
max_seq_len = 2048

# LoRA parameters
use_lora = true
lora_rank = 8
lora_alpha = 32
lora_dropout = 0.0

# Model parameters
use_liger = true
bf16 = true
gradient_checkpointing = false

# Logging and saving
output_dir = "outputs/wordle-sft"
logging_steps = 1
save_steps = 100
save_strategy = "steps"
eval_strategy = "no"
report_to = "wandb"

# Optional vLLM integration for sample generation
use_vllm = false
# vllm_sample_every_n_steps = 100
# vllm_num_samples = 5
# vllm_server_host = "0.0.0.0"
# vllm_server_port = 8000
